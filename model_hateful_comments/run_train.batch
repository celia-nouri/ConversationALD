#!/bin/bash

# Launch SLURM parameters

#SBATCH --cpus-per-gpu=8        # Number of cpus per GPU card (>1 if multi-threaded tasks)
#SBATCH --partition=almanach        # Name of the partition
#SBATCH --account=almanach
#SBATCH --gres=gpu:rtx8000:2    # Number and type of GPU cards and type allocated
#SBATCH --mem=20G                # Total memory allocated
#SBATCH --time=12:45:00 # total run time limit (HH:MM:SS)
#SBATCH --output=%x_%j.out       # output file name


#SBATCH --job-name=hd_experiment    # create a short name for your job
#SBATCH --hint=multithread       # we get physical cores not logical


#SBATCH --error=%x_%j.err    # error file name
#SBATCH --mail-type=ALL
#SBATCH --mail-user=celia.nouri@inria.fr


echo "### Running $SLURM_JOB_NAME ###"

set -x

export SLURM_TMPDIR=$(pwd)
export src=$(pwd)


export WANDB_NAME=hd-hate--$(date +%D)--$(hostname)--${RANDOM}
export WANDB_ENTITY='celia-nouri'
export WANDB_PROJECT='hatespeech-class'


module purge
module load cuda/11.4.0


cd $SLURM_TMPDIR
cd $src 

# Set your conda environment
source /home/$USER/.bashrc
# conda environment should be created previously
conda activate hatedisc


cd $SLURM_TMPDIR 

echo "starting the training..."

python experiments.py
