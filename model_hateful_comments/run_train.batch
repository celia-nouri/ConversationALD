#!/bin/bash

# Launch SLURM parameters

#SBATCH --cpus-per-gpu=16        # Number of cpus per GPU card (>1 if multi-threaded tasks)
#SBATCH --partition=gpu   # Name of the partition 
#SBATCH --gres=gpu:3   # Number and type of GPU cards and type allocated rtx8000
#SBATCH --mem=4096G                # Total memory allocated
#SBATCH --time=48:00:00 # total run time limit (HH:MM:SS)
#SBATCH --output=%x_%j.out       # output file name
#SBATCH --mem=0  # Request all available memory on the node

#SBATCH --job-name=bot-gat-dir-1l-cad-512-99  # create a short name for your job
#SBATCH --hint=multithread       # we get physical cores not logical

#SBATCH --error=%x_%j.err    # error file name
#SBATCH --mail-type=ALL
#SBATCH --mail-user=celia.nouri@inria.fr

echo "### Running $SLURM_JOB_NAME ###"

set -x

export SLURM_TMPDIR=$(pwd)
export src=$(pwd)


export WANDB_NAME=hd-hate--$(date +%D)--$(hostname)--${RANDOM}
export WANDB_ENTITY='celia-nouri'
export WANDB_PROJECT='hatespeech-class'


module purge
module load cuda/11.4.0


cd $SLURM_TMPDIR
cd $src 

# Set your conda environment
source /home/$USER/.bashrc
# conda environment should be created previously
conda activate hatedisc

export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
echo $PYTORCH_CUDA_ALLOC_CONF

cd $SLURM_TMPDIR 

echo "starting the training..."

python experiments.py
