#!/bin/bash

# Launch SLURM parameters

#SBATCH --cpus-per-gpu=16        # Number of cpus per GPU card (>1 if multi-threaded tasks)
#SBATCH --partition=almanach    # Name of the partition
#SBATCH --account=almanach 
#SBATCH --gres=gpu:3      # Number and type of GPU cards and type allocated
#SBATCH --mem=512G                # Total memory allocated
#SBATCH --time=48:00:00 # total run time limit (HH:MM:SS)
#SBATCH --output=%x_%j.out       # output file name
#SBATCH --mem=0  # Request all available memory on the node

#SBATCH --job-name=eval-otrim-gat-undir-3l-max512-cad   # create a short name for your job
#SBATCH --hint=multithread       # we get physical cores not logical


#SBATCH --error=%x_%j.err    # error file name
#SBATCH --mail-type=ALL
#SBATCH --mail-user=celia.nouri@inria.fr


echo "### Running $SLURM_JOB_NAME ###"

set -x

export SLURM_TMPDIR=$(pwd)
export src=$(pwd)


export WANDB_NAME=eval--$(date +%D)--$(hostname)--${RANDOM}
export WANDB_ENTITY='celia-nouri'
export WANDB_PROJECT='hatespeech-class'


module purge
module load cuda/11.4.0


cd $SLURM_TMPDIR
cd $src 

# Set your conda environment
source /home/$USER/.bashrc
# conda environment should be created previously
conda activate hatedisc

export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
echo $PYTORCH_CUDA_ALLOC_CONF

cd $SLURM_TMPDIR 

echo "starting the evaluation..."

python main_evaluate.py
